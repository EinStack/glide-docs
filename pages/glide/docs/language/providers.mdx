---
title: 'Providers'
description: 'Supported Language Providers'
---
# Providers

Here is a list of supported language providers:
- [OpenAI](#openai)
- [Azure OpenAI](#azure-openai)
- [Anthropic](#anthropic)
- [Cohere](#cohere)
- [AWS Bedrock](#aws-bedrock)
- [OctoML](#octoml)
- [Ollama](#ollama)
- [more coming](https://github.com/EinStack/glide/issues?q=is:issue+is:open+label:model:language)

## OpenAI

[OpenAI](https://openai.com/) is a super popular LLM provider that basically created the GenAI movement.

Below is an example OpenAI provider config:

```yaml
routers:
  language:
    - id: default
      models:
        - id: openai
          openai:
            base_url: https://api.openai.com/v1
            chat_endpoint: /chat/completions
            model: gpt-3.5-turbo
            api_key: <YOUR API KEY>
            default_params:
                temperature: 0.8
                top_p: 1
                max_tokens: 100
                n: 1
                frequency_penalty: 0
                presence_penalty: 0
                seed: 42
```

Here is a list of all supported provider model params:
- [Github](https://github.com/EinStack/glide/blob/main/pkg/providers/openai/config.go#L9)

## Azure OpenAI

[Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) is an Azure-hosted version of OpenAI models.

Below is an example Azure OpenAI provider config:

```yaml
routers:
  language:
    - id: default
      models:
        - id: azureopenai
          azureopenai:
            base_url: <YOUR AZURE ENDPOINT>
            chat_endpoint: /chat/completions
            api_version: "2023-05-15"
            model: gpt-3.5-turbo
            api_key: <YOUR API KEY>
            default_params:
                temperature: 0.8
                top_p: 1
                max_tokens: 100
                n: 1
                frequency_penalty: 0
                presence_penalty: 0
                seed: 42
```

Here is a list of all supported provider model params:
- [Github](https://github.com/EinStack/glide/blob/main/pkg/providers/azureopenai/config.go#L9)

## Anthropic

[Anthropic](https://www.anthropic.com/)

Below is an example Anthropic provider config:

```yaml
routers:
  language:
    - id: default
      models:
        - id: anthropic
          anthropic:
            base_url: https://api.anthropic.com/v1
            api_version: "2023-06-01"
            chat_endpoint: /messages
            model: claude-instant-1.2
            api_key: <YOUR API KEY>
            default_params:
                system: You are a helpful assistant.
                temperature: 1
                max_tokens: 250
```

Here is a list of all supported provider model params:
- [Github](https://github.com/EinStack/glide/blob/main/pkg/providers/anthropic/config.go#L9)

## Cohere

[Cohere](https://cohere.com/) is another popular LLM provider that has a great low latency models.

Here is an example Cohere configuration:

```yaml
routers:
  language:
    - id: default
      models:
        - id: cohere
          cohere:
            base_url: https://api.cohere.ai/v1
            chat_endpoint: /chat
            model: command-light
            api_key: <YOUR API KEY>
            default_params:
                temperature: 0.3
                p: 0.75
```

Here is a list of all supported provider model params:
- [Github](https://github.com/EinStack/glide/blob/main/pkg/providers/cohere/config.go#L9)

## AWS Bedrock

```yaml
```

## OctoML

[OctoML](https://octo.ai/) the default_params and model name for OctoML. Specify override values in the `config.yaml` file.

```yaml
model: "mistral-7b-instruct-fp16"
default_params:
  temperature: 1 # number between 0 - 2
  top_p: 1
  max_tokens: 100
  stop:
    - ""
  frequency_penalty: 0
  presence_penalty: 0
```

## Ollama

[]()

```yaml
```
